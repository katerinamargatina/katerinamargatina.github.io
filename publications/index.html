<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  katerina margatina


  | publications

</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>ü¶ã</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/publications/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-XXXXXXXXX');
</script>



<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=XXXXXXXXX"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'XXXXXXXXX' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <a class="navbar-brand title font-weight-lighter" href="/">
       katerina margatina
      </a>
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item ">
            <a class="nav-link" href="/">
              about
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item active">
              <a class="nav-link" href="/publications/">
                publications
                
                <span class="sr-only">(current)</span>
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/vitae/">
                vitae
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">publications</h1>
    <p class="post-description"></p>
  </header>

  <article>
    <p><a href="">*</a> denotes equal contribution</p>

<p>An up-to-date list is available on <a href="https://scholar.google.com/citations?user=517t5gEAAAAJ">Google Scholar</a>.</p>

<div class="publications">


  <h2 class="year">2024</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">Thesis</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="thesis" class="col-sm-8">
        
        <div class="title">Exploring Active Learning Algorithms for Data Efficient Language Models</div>
        <div class="author">
            
            
            
            <em>Katerina Margatina</em>
            
            
            
        </div>

        <div class="periodical">
            
            
            2024
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://etheses.whiterose.ac.uk/35849/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Supervised learning is based in the premise that models can effectively solve tasks by learning from numerous examples, mapping inputs to outputs through iterative learning. However, contemporary deep learning models often require vast amounts of labeled data, termed training examples, for optimal performance. Unfortunately, not all training examples contribute equally to the learning process, leading to inefficiencies and resource wastage. Active Learning (AL) has emerged as a powerful paradigm for training language models in a data-efficient manner. By iteratively selecting informative unlabeled data points, which are then annotated by humans to form the training set, AL intelligently guides the training process, optimizing data selection for model improvement over random sampling. This thesis investigates various aspects of active learning algorithms for language mod- els, focusing on model training, data selection, in-context learning and simulation. The thesis is structured along four key publications that tackle these topics respectively. The first publication addresses the effective adaptation of pretrained language models for AL, highlighting the importance of task-specific fine-tuning. The second publication introduces a novel acquisition function, Contrastive Active Learning (CAL), which selects contrastive examples to improve AL performance. The third publication explores active learning principles for in-context learning with large language models, emphasizing the selection of informative demonstrations for few-shot learning. Lastly, the fourth publication critically examines the limitations of simulating AL experiments and pro- poses guidelines for future research. Through these contributions, this thesis aims to advance our understanding of AL algorithms for data-efficient language model training.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">NeurIPS</abbr>
        
        

        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;"> üèÜ Best Paper</span>
        </p>
        
<!--        -->
<!--        <p>-->
<!--            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i>-->
<!--            <span  style="font-size: 0.8em;" > üèÜ Best Paper</span>-->
<!--        </p>-->
<!--        -->

        
    </div>

    <div id="prism" class="col-sm-8">
        
        <div class="title">The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</div>
        <div class="author">
            
            
            
            
            
            Hannah Rose Kirk,
            
            
            
            
            
            
            
            
            
            Alexander Whitefield,
            
            
            
            
            
            
            
            
            
            Paul R√∂ttger,
            
            
            
            
            
            
            
            
            
            Andrew Bean,
            
            
            
            
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Juan Ciro,
            
            
            
            
            
            
            
            
            
            Rafael Mosquera,
            
            
            
            
            
            
            
            
            
            Max Bartolo,
            
            
            
            
            
            
            
            
            
            Adina Williams,
            
            
            
            
            
            
            
            
            
            Bertie Vidgen He He,
            
            
            
            
            
            
            
            
            
            and Scott A. Hale
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks.</em>
            
            
            2024
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2404.16019" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://openreview.net/forum?id=DFr5hteojx#discussion" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            
            <a href="https://mlcommons.org/2024/05/prism/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            <a href="https://x.com/hannahrosekirk/status/1783470502595440884" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.</p>
        </div>
        
    </div>
</div></li></ol>

  <h2 class="year">2023</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EMNLP-Findings</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="margatina2023active" class="col-sm-8">
        
        <div class="title">Active Learning Principles for In-Context Learning with Large Language Models</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Timo Schick,
            
            
            
            
            
            
            
            
            
            Nikolaos Aletras,
            
            
            
            
            
            
            
            
            
            and Jane Dwivedi-Yu
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2023</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2305.14264" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            
            
            
            
            <a href="https://twitter.com/katemargatina/status/1661321935127556097" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>The remarkable advancements in large language models (LLMs) have significantly enhanced the performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively grasp the task at hand through in-context learning. However, the process of selecting appropriate demonstrations has received limited attention in prior work. This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration. Our objective is to investigate how AL algorithms can serve as effective demonstration selection methods for in-context learning. We compare various standard AL algorithms based on uncertainty, diversity, and similarity, and consistently observe that the latter outperforms all other methods, including random sampling. Notably, uncertainty sampling, despite its success in conventional supervised learning scenarios, performs poorly in this context. Our extensive experimentation involving a diverse range of GPT and OPT models across 24 classification and multi-choice tasks, coupled with thorough analysis, unambiguously demonstrates that in-context example selection through AL prioritizes high-quality examples that exhibit low uncertainty and bear similarity to the test examples.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="ahmed2023" class="col-sm-8">
        
        <div class="title">Understanding the Role of Input Token Characters in Language Models: How Does Information Loss Affect Performance?</div>
        <div class="author">
            
            
            
            
            
            Ahmed Alajrami,
            
            
            
            
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            and Nikolaos Aletras
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2310.17271" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            
            
            
            
            <a href="https://twitter.com/nikaletras/status/1717915350799437865" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Understanding how and what pre-trained language models (PLMs) learn about language is an open challenge in natural language processing. Previous work has focused on identifying whether they capture semantic and syntactic information, and how the data or the pre-training objective affects their performance. However, to the best of our knowledge, no previous work has specifically examined how information loss in input token characters affects the performance of PLMs. In this study, we address this gap by pre-training language models using small subsets of characters from individual tokens. Surprisingly, we find that pre-training even under extreme settings, i.e. using only one character of each token, the performance retention in standard NLU benchmarks and probing tasks compared to full-token models is high. For instance, a model pre-trained only on single first characters from tokens achieves performance retention of approximately 90% and 77% of the full-token model in SuperGLUE and GLUE tasks, respectively.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">ACL-Findings</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="margatina2023limitations" class="col-sm-8">
        
        <div class="title">On the Limitations of Simulating Active Learning</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            and Nikolaos Aletras
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Findings of the Association for Computational Linguistics (ACL)</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2305.13342" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2023.findings-acl.269/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2023.findings-acl.269.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question ‚Äúwhy do active learning algorithms sometimes fail to outperform random sampling?‚Äù. We argue that evaluating AL algorithms on available labeled datasets might provide a lower bound as to their effectiveness in real data. We believe it is essential to collectively shape the best practices for AL research, particularly as engineering advancements in LLMs push the research focus towards data-driven approaches (e.g., data efficiency, alignment, fairness). In light of this, we have developed guidelines for future work. Our aim is to draw attention to these limitations within the community, in the hope of finding ways to address them.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EACL</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="dynamic_templama" class="col-sm-8">
        
        <div class="title">Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Shuai Wang,
            
            
            
            
            
            
            
            
            
            Yogarshi Vyas,
            
            
            
            
            
            
            
            
            
            Neha Anna John,
            
            
            
            
            
            
            
            
            
            Yassine Benajiba,
            
            
            
            
            
            
            
            
            
            and Miguel Ballesteros
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the European Meeting of the Association for Computational Linguistics (EACL)</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2302.12297" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2023.eacl-main.211/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2023.eacl-main.211.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            <a href="https://www.amazon.science/publications/dynamic-benchmarking-of-masked-language-models-on-temporal-concept-drift-with-multiple-views" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            <a href="https://twitter.com/katemargatina/status/1653535912532344833" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            <a href="https://github.com/amazon-science/temporal-robustness" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            <a href="https://drive.google.com/file/d/1k_AxteMtoFfXt-ReX1VYxz1tME8InEW_/view?usp=share_link" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
            
            
            
            
            <a href="https://drive.google.com/file/d/1Ln29c02nd2iGMv7diajdacDs5OXNxMyl/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Temporal concept drift refers to the problem of data changing over time. In the field of NLP, that would entail that language (e.g. new expressions, meaning shifts) and factual knowledge (e.g. new concepts, updated facts) evolve over time. Focusing on the latter, we benchmark 11 pretrained masked language models (MLMs) on a series of tests designed to evaluate the effect of temporal concept drift, as it is crucial that widely used language models remain up-to-date with the ever-evolving factual updates of the real world. Specifically, we provide a holistic framework that (1) dynamically creates temporal test sets of any time granularity (e.g. month, quarter, year) of factual data from Wikidata, (2) constructs fine-grained splits of tests (e.g. updated, new, unchanged facts) to ensure comprehensive analysis, and (3) evaluates MLMs in three distinct ways (single-token probing, multi-token generation, MLM scoring). In contrast to prior work, our framework aims to unveil how robust an MLM is over time and thus to provide a signal in case it has become outdated, by leveraging multiple views of evaluation.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EACL</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="al_nli" class="col-sm-8">
        
        <div class="title">Investigating Multi-source Active Learning for Natural Language Inference</div>
        <div class="author">
            
            
            
            
            
            Ard Snijders,
            
            
            
            
            
            
            
            
            
            Douwe Kiela,
            
            
            
            
            
            
            
            
            and <em>Katerina Margatina</em>
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the European Meeting of the Association for Computational Linguistics (EACL)</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2302.06976" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2023.eacl-main.160/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2023.eacl-main.160.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://twitter.com/katemargatina/status/1628714831409778688" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            <a href="https://github.com/asnijders/multi_source_AL" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            <a href="https://drive.google.com/file/d/1_a6N6l-KYcpAGEUuEVgjXdXJ8SCdpK3c/view?usp=share_link" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In recent years, active learning has been successfully applied to an array of NLP tasks. However, prior work often assumes that training and test data are drawn from the same distribution. This is problematic, as in real-life settings data may stem from several sources of varying relevance and quality. We show that four popular active learning schemes fail to outperform random selection when applied to unlabelled pools comprised of multiple data sources on the task of natural language inference. We reveal that uncertainty-based strategies perform poorly due to the acquisition of collective outliers, i.e., hard-to-learn instances that hamper learning and generalisation. When outliers are removed, strategies are found to recover and outperform random baselines. In further analysis, we find that collective outliers vary in form between sources, and show that hard-to-learn data is not always categorically harmful. Lastly, we leverage dataset cartography to introduce difficulty-stratified testing and find that different strategies are affected differently by example learnability and difficulty.</p>
        </div>
        
    </div>
</div></li></ol>

  <h2 class="year">2022</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">DADC@NAACL</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="dadc-2022-dynamic" class="col-sm-8">
        
        <div class="title">Proceedings of the First Workshop on Dynamic Adversarial Data Collection</div>
        <div class="author">
            
            
            
            
            
            Max Bartolo,
            
            
            
            
            
            
            
            
            
            Hannah Kirk,
            
            
            
            
            
            
            
            
            
            Pedro Rodriguez,
            
            
            
            
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Tristan Thrush,
            
            
            
            
            
            
            
            
            
            Robin Jia,
            
            
            
            
            
            
            
            
            
            Pontus Stenetorp,
            
            
            
            
            
            
            
            
            
            Adina Williams,
            
            
            
            
            
            
            
            
            
            and Douwe Kiela
            
            
            
            
            
        </div>

        <div class="periodical">
            
            
            2022
            
        </div>
        

        <div class="links">
            
            
            
            <a href="https://dadcworkshop.github.io/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2022.dadc-1.0" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">ACL</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="ex-balm" class="col-sm-8">
        
        <div class="title">On the Importance of Effectively Adapting Pretrained Language Models for Active Learning</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Lo√Øc Barrault,
            
            
            
            
            
            
            
            
            
            and Nikolaos Aletras
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</em>
            
            
            2022
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2104.08320" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2022.acl-short.93/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2022.acl-short.93.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://twitter.com/katemargatina/status/1508445688333582341" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            <a href="https://github.com/mourga/contrastive-active-learning" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            <a href="https://drive.google.com/file/d/1z6jJsZOb8uYbqZWFsyQHlXOisBkruN_t/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
            
            
            
            
            <a href="https://drive.google.com/file/d/1Gn-a0s2sA2duH5PyFi-JS6MM3EqkM69O/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Recent Active Learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">ACL</abbr>
        
        

        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;">  üåç Theme üåé</span>
        </p>
        
<!--        -->
<!--        <p>-->
<!--            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i>-->
<!--            <span  style="font-size: 0.8em;" >  üåç Theme üåé</span>-->
<!--        </p>-->
<!--        -->

        
    </div>

    <div id="xcultural" class="col-sm-8">
        
        <div class="title">Challenges and Strategies in Cross-Cultural NLP</div>
        <div class="author">
            
            
            
            
            
            Daniel Hershcovich,
            
            
            
            
            
            
            
            
            
            Stella Frank,
            
            
            
            
            
            
            
            
            
            Heather Lent,
            
            
            
            
            
            
            
            
            
            Miryam Lhoneux,
            
            
            
            
            
            
            
            
            
            Mostafa Abdou,
            
            
            
            
            
            
            
            
            
            Stephanie Brandl,
            
            
            
            
            
            
            
            
            
            Emanuele Bugliarello,
            
            
            
            
            
            
            
            
            
            Laura Cabello Piqueras,
            
            
            
            
            
            
            
            
            
            Ilias Chalkidis,
            
            
            
            
            
            
            
            
            
            Ruixiang Cui,
            
            
            
            
            
            
            
            
            
            Constanza Fierro,
            
            
            
            
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Phillip Rust,
            
            
            
            
            
            
            
            
            
            and Anders S√∏gaard
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</em>
            
            
            2022
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2203.10020" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2022.acl-long.482/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2022.acl-long.482.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://twitter.com/daniel_hers/status/1505829084210868224" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies.</p>
        </div>
        
    </div>
</div></li></ol>

  <h2 class="year">2021</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        

        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span style="font-size: 0.8em;">  ‚ú® Oral ‚ú®</span>
        </p>
        
<!--        -->
<!--        <p>-->
<!--            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i>-->
<!--            <span  style="font-size: 0.8em;" >  ‚ú® Oral ‚ú®</span>-->
<!--        </p>-->
<!--        -->

        
    </div>

    <div id="cal" class="col-sm-8">
        
        <div class="title">Active Learning by Acquiring Contrastive Examples</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Giorgos Vernikos,
            
            
            
            
            
            
            
            
            
            Lo√Øc Barrault,
            
            
            
            
            
            
            
            
            
            and Nikolaos Aletras
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2109.03764" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2021.emnlp-main.51/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2021.emnlp-main.51.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://twitter.com/katemargatina/status/1437393852227276801?s=20" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            <a href="https://github.com/mourga/contrastive-active-learning" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            <a href="https://drive.google.com/file/d/1JAdukTFb0ceAR0WRk9zEW2GqLiTG_p7H/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
            
            
            
            
            <a href="https://drive.google.com/file/d/1bWx9QPRTFjf49XLgCdV34w3w1SXETylv/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \textitcontrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="frustratingly" class="col-sm-8">
        
        <div class="title">Frustratingly Simple Alternatives to Masked Language Modeling</div>
        <div class="author">
            
            
            
            
            
            Atsuki Yamaguchi,
            
            
            
            
            
            
            
            
            
            George Chrysostomou,
            
            
            
            
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            and Nikolaos Aletras
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2109.01819" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2021.emnlp-main.249/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2021.emnlp-main.249.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://twitter.com/nikaletras/status/1435206594623574019?s=20" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            <a href="https://github.com/gucci-j/light-transformer-emnlp2021" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE‚Äôs parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.</p>
        </div>
        
    </div>
</div></li></ol>

  <h2 class="year">2020</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EMNLP-Findings</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="vernikos-etal-2020-domain" class="col-sm-8">
        
        <div class="title">Domain Adversarial Fine-Tuning as an Effective Regularizer</div>
        <div class="author">
            
            
            
            
            
            Giorgos Vernikos,
            
            
            
            
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Alexandra Chronopoulou,
            
            
            
            
            
            
            
            
            
            and Ion Androutsopoulos
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Findings of the Association for Computational Linguistics (EMNLP)</em>
            
            
            2020
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/2020.findings-emnlp.278" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2020.findings-emnlp.278.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://twitter.com/gvernikos/status/1311010294735482880?s=20" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            <a href="https://github.com/GeorgeVern/AFTERV1.0" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. Indomain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularize which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.</p>
        </div>
        
    </div>
</div></li></ol>

  <h2 class="year">2019</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">Thesis</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="thesit" class="col-sm-8">
        
        <div class="title">Transfer Learning and Attention-based Conditioning Methods for Natural Language Processing</div>
        <div class="author">
            
            
            
            <em>Katerina Margatina</em>
            
            
            
        </div>

        <div class="periodical">
            
            <em></em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            
            
            
            
            <a href="http://artemis.cslab.ece.ntua.gr:8080/jspui/bitstream/123456789/17295/1/Eng_Thesis_Kate.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            
            
            
            
            <a href="https://drive.google.com/open?id=1CkV6TfdObiQfozh7a4dH_pmJFgxvr2BH" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">ACL</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="margatina-etal-2019-attention" class="col-sm-8">
        
        <div class="title">Attention-based Conditioning Methods for External Knowledge Integration</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Christos Baziotis,
            
            
            
            
            
            
            
            
            
            and Alexandros Potamianos
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)</em>
            
            
            2019
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://www.aclweb.org/anthology/P19-1385" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/P19-1385.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            
            <a href="https://github.com/mourga/affective-attention" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            <a href="https://drive.google.com/file/d/1Z0xZ_y_oJKpeeXZ_8z2-sOQJZIQdkagZ/view" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.</p>
        </div>
        
    </div>
</div></li></ol>

  <h2 class="year">2018</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">WASSA@EMNLP</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="W18-6209" class="col-sm-8">
        
        <div class="title">NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification</div>
        <div class="author">
            
            
            
            
            
            Alexandra* Chronopoulou,
            
            
            
            
            
            
            
            
            <em>Katerina* Margatina</em>,
            
            
            
            
            
            
            
            
            Christos Baziotis,
            
            
            
            
            
            
            
            
            
            and Alexandros Potamianos
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)</em>
            
            
            2018
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="http://aclweb.org/anthology/W18-6209" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/W18-6209.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            
            <a href="https://github.com/alexandra-chron/ntua-slp-wassa-iest2018" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            
            <a href="https://drive.google.com/file/d/1hyUx69rVEzyFuBeZPtVWz8_JSb83d7jJ/view" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, augmented with a self-attention mechanism. We use the weights of various pretrained models, for initializing specific layers of our networks. We leverage a big collection of unlabeled Twitter messages, for pretraining word2vec word embeddings and a set of diverse language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703.</p>
        </div>
        
    </div>
</div></li></ol>


</div>


  </article>

</div>

    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Katerina  Margatina.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
