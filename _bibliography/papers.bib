---
---

@string{aps = {American Physical Society,}}

%--------------------------------------------------------------------------
% 2023
%--------------------------------------------------------------------------
@proceedings{dynamic_templama,
abbr={EACL},
    title = "Dynamic Benchmarking of Masked Language Models on Temporal Concept Drift with Multiple Views",
      author = {Margatina, Katerina  and
      Wang, Shuai  and
      Vyas, Yogarshi  and
      John, Neha Anna  and
      Benajiba, Yassine  and
      Ballesteros, Miguel},
      booktitle = "Proceedings of the European Meeting of the Association for Computational Linguistics (EACL)",
      selected={true},
    year = "2023",
}

@proceedings{al_nli,
abbr={EACL},
    title = "Investigating Multi-source Active Learning for Natural Language Inference",
      author = {Snijders, Ard  and
      Kiela, Douwe  and
      Margatina, Katerina},
      booktitle = "Proceedings of the European Meeting of the Association for Computational Linguistics (EACL)",
      selected={true},
    year = "2023",
}
%--------------------------------------------------------------------------
% 2022
%--------------------------------------------------------------------------
@proceedings{dadc-2022-dynamic,
abbr={DADC@NAACL},
    title = "Proceedings of the First Workshop on Dynamic Adversarial Data Collection",
    editor = "Bartolo, Max  and
      Kirk, Hannah  and
      Rodriguez, Pedro  and
      Margatina, Katerina  and
      Thrush, Tristan  and
      Jia, Robin  and
      Stenetorp, Pontus  and
      Williams, Adina  and
      Kiela, Douwe",
      author = {M. Bartolo  and
      H. Kirk  and
      P. Rodriguez  and
      K. Margatina  and
      T. Thrush  and
      R. Jia  and
      P. Stenetorp  and
      A. Williams  and
      D. Kiela},
    month = jul,
    year = "2022",
    address = "Seattle, WA",
    publisher = "Association for Computational Linguistics",
    url = "https://aclanthology.org/2022.dadc-1.0",
    html = "https://dadcworkshop.github.io/",
    pdf = "https://aclanthology.org/2022.dadc-1.0"
}

@inproceedings{ex-balm,
abbr={ACL},
      title={On the Importance of Effectively Adapting Pretrained Language Models for Active Learning},
      author={K. Margatina
       and L. Barrault and N. Aletras},
      year={2022},
      booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)",
      selected={true},
      code={https://github.com/mourga/contrastive-active-learning},
      arxiv={2104.08320},
      slides={https://drive.google.com/file/d/1Gn-a0s2sA2duH5PyFi-JS6MM3EqkM69O/view?usp=sharing},
      poster={https://drive.google.com/file/d/1z6jJsZOb8uYbqZWFsyQHlXOisBkruN_t/view?usp=sharing},
      tldr={https://twitter.com/katemargatina/status/1508445688333582341},
      html = "https://aclanthology.org/2022.acl-short.93/",
      pdf = "https://aclanthology.org/2022.acl-short.93.pdf",
      abstract={Recent Active Learning (AL) approaches in Natural Language Processing (NLP) proposed using off-the-shelf pretrained language models (LMs). In this paper, we argue that these LMs are not adapted effectively to the downstream task during AL and we explore ways to address this issue. We suggest to first adapt the pretrained LM to the target task by continuing training with all the available unlabeled data and then use it for AL. We also propose a simple yet effective fine-tuning method to ensure that the adapted LM is properly trained in both low and high resource scenarios during AL. Our experiments demonstrate that our approach provides substantial data efficiency improvements compared to the standard fine-tuning approach, suggesting that a poor training strategy can be catastrophic for AL.}
}

@inproceedings{xcultural,
abbr={ACL},
award={üåç Theme üåé},
      title={Challenges and Strategies in Cross-Cultural NLP},
      author={D. Hershcovich and S. Frank and H. Lent and M. de Lhoneux and M. Abdou and S. Brandl and E. Bugliarello and L. Cabello Piqueras and I. Chalkidis
       and R. Cui and C. Fierro and K. Margatina and P. Rust and A. S√∏gaard},
      year={2022},
      booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)",
      pdf = "https://danielhers.github.io/xculture.pdf",
      arxiv = {2203.10020},
      tldr = {https://twitter.com/daniel_hers/status/1505829084210868224},
      html = "https://aclanthology.org/2022.acl-long.482/",
      pdf = "https://aclanthology.org/2022.acl-long.482.pdf",
      abstract = "Various efforts in the Natural Language Processing (NLP) community have been made to accommodate linguistic diversity and serve speakers of many different languages. However, it is important to acknowledge that speakers and the content they produce and require, vary not just by language, but also by culture. Although language and culture are tightly linked, there are important differences. Analogous to cross-lingual and multilingual NLP, cross-cultural and multicultural NLP considers these differences in order to better serve users of NLP systems. We propose a principled framework to frame these efforts, and survey existing and potential strategies."
}
%--------------------------------------------------------------------------
% 2021
%--------------------------------------------------------------------------
@inproceedings{cal,
abbr={EMNLP},
award={‚ú® Oral ‚ú®},
      title={Active Learning by Acquiring Contrastive Examples},
      author={K. Margatina and G. Vernikos
       and L. Barrault and N. Aletras},
      year={2021},
      booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      selected={true},
      code={https://github.com/mourga/contrastive-active-learning},
      arxiv={2109.03764},
      html = "https://aclanthology.org/2021.emnlp-main.51/",
      pdf = "https://aclanthology.org/2021.emnlp-main.51.pdf",
      pages = "650--663",
      tldr={https://twitter.com/katemargatina/status/1437393852227276801?s=20},
      slides = {https://drive.google.com/file/d/1bWx9QPRTFjf49XLgCdV34w3w1SXETylv/view?usp=sharing},
      poster = {https://drive.google.com/file/d/1JAdukTFb0ceAR0WRk9zEW2GqLiTG_p7H/view?usp=sharing},
      abstract={Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \textit{contrastive examples}, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.}
}
@inproceedings{frustratingly,
abbr={EMNLP},
      title={Frustratingly Simple Alternatives to Masked Language Modeling},
      author={A. Yamaguchi and G. Chrysostomou and K. Margatina and N. Aletras},
      year={2021},
      booktitle = "Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)",
      code={https://github.com/gucci-j/light-transformer-emnlp2021},
      arxiv={2109.01819},
      html = "https://aclanthology.org/2021.emnlp-main.249/",
      pages = "3116--3125",
      pdf = "https://aclanthology.org/2021.emnlp-main.249.pdf",
      pages = "650--663",
      tldr={https://twitter.com/nikaletras/status/1435206594623574019?s=20},
      abstract={Masked language modeling (MLM), a self-supervised pretraining objective, is widely used in natural language processing for learning text representations. MLM trains a model to predict a random sample of input tokens that have been replaced by a [MASK] placeholder in a multi-class setting over the entire vocabulary. When pretraining, it is common to use alongside MLM other auxiliary objectives on the token or sequence level to improve downstream performance (e.g. next sentence prediction). However, no previous work so far has attempted in examining whether other simpler linguistically intuitive or not objectives can be used standalone as main pretraining objectives. In this paper, we explore five simple pretraining objectives based on token-level classification tasks as replacements of MLM. Empirical results on GLUE and SQuAD show that our proposed methods achieve comparable or better performance to MLM using a BERT-BASE architecture. We further validate our methods using smaller models, showing that pretraining a model with 41% of the BERT-BASE's parameters, BERT-MEDIUM results in only a 1% drop in GLUE scores with our best objective.}
}

%--------------------------------------------------------------------------
% 2020
%--------------------------------------------------------------------------
@inproceedings{vernikos-etal-2020-domain,
    abbr = {EMNLP-Findings},
    title = "{D}omain {A}dversarial {F}ine-{T}uning as an {E}ffective {R}egularizer",
    author = "G. Vernikos, and
      K. Margatina, and
      A. Chronopoulou,  and
      I. Androutsopoulos",
    booktitle = "Findings of the Association for Computational Linguistics (EMNLP)",
    year = "2020",
    html = "https://www.aclweb.org/anthology/2020.findings-emnlp.278",
    pdf = "https://aclanthology.org/2020.findings-emnlp.278.pdf",
    pages = "3103--3112",
    code = {https://github.com/GeorgeVern/AFTERV1.0},
    abstract = "In Natural Language Processing (NLP), pretrained language models (LMs) that are transferred to downstream tasks have been recently shown to achieve state-of-the-art results. However, standard fine-tuning can degrade the general-domain representations captured during pretraining. To address this issue, we introduce a new regularization technique, AFTER; domain Adversarial Fine-Tuning as an Effective Regularizer. Specifically, we complement the task-specific loss used during fine-tuning with an adversarial objective. This additional loss term is related to an adversarial classifier, that aims to discriminate between in-domain and out-of-domain text representations. Indomain refers to the labeled dataset of the task at hand while out-of-domain refers to unlabeled data from a different domain. Intuitively, the adversarial classifier acts as a regularize which prevents the model from overfitting to the task-specific domain. Empirical results on various natural language understanding tasks show that AFTER leads to improved performance compared to standard fine-tuning.",
    tldr= "https://twitter.com/gvernikos/status/1311010294735482880?s=20"
}

%--------------------------------------------------------------------------
% 2019
%--------------------------------------------------------------------------
@article{thesis,
  abbr = {Thesis},
  author = 	"Margatina, Katerina",
  title = 	{Transfer Learning and Attention-based Conditioning Methods for Natural Language Processing},
  journaltitle = 	"Thesis, NTUA",
  year = 	"2019",
  date={2019},
  pdf = 	"http://artemis.cslab.ece.ntua.gr:8080/jspui/bitstream/123456789/17295/1/Eng_Thesis_Kate.pdf",
  slides = {https://drive.google.com/open?id=1CkV6TfdObiQfozh7a4dH_pmJFgxvr2BH}
}

@inproceedings{margatina-etal-2019-attention,
    abbr = {ACL},
    title = "Attention-based Conditioning Methods for External Knowledge Integration",
    author = "K. Margatina,  and
      C. Baziotis,  and
      A. Potamianos",
    booktitle = "Proceedings of the Annual Meeting of the Association for Computational Linguistics (ACL)",
    year = "2019",
    address = "Florence, Italy",
    html = {https://www.aclweb.org/anthology/P19-1385},
    pdf = "https://aclanthology.org/P19-1385.pdf",
    pages = "3944--3951",
    code = {https://github.com/mourga/affective-attention},
    poster = {https://drive.google.com/file/d/1Z0xZ_y_oJKpeeXZ_8z2-sOQJZIQdkagZ/view},
    abstract = "In this paper, we present a novel approach for incorporating external knowledge in Recurrent Neural Networks (RNNs). We propose the integration of lexicon features into the self-attention mechanism of RNN-based architectures. This form of conditioning on the attention distribution, enforces the contribution of the most salient words for the task at hand. We introduce three methods, namely attentional concatenation, feature-based gating and affine transformation. Experiments on six benchmark datasets show the effectiveness of our methods. Attentional feature-based gating yields consistent performance improvement across tasks. Our approach is implemented as a simple add-on module for RNN-based models with minimal computational overhead and can be adapted to any deep neural architecture.",
}



%--------------------------------------------------------------------------
% 2018
%--------------------------------------------------------------------------
@inproceedings{W18-6209,
  abbr = {WASSA@EMNLP},
  author = 	"A.* Chronopoulou,
		and K.* Margatina,
		and C. Baziotis,
		and A. Potamianos",
  title = 	"NTUA-SLP at IEST 2018: Ensemble of Neural Transfer Methods for Implicit Emotion Classification",
  booktitle = 	"Proceedings of the 9th Workshop on Computational Approaches to Subjectivity, Sentiment and Social Media Analysis (WASSA)",
  year = 	"2018",
  pdf = 	"https://aclanthology.org/W18-6209.pdf",
  html = 	"http://aclweb.org/anthology/W18-6209",
  code={https://github.com/alexandra-chron/ntua-slp-wassa-iest2018},
  slides={https://drive.google.com/file/d/1hyUx69rVEzyFuBeZPtVWz8_JSb83d7jJ/view},
  abstract = {In this paper we present our approach to tackle the Implicit Emotion Shared Task (IEST) organized as part of WASSA 2018 at EMNLP 2018. Given a tweet, from which a certain word has been removed, we are asked to predict the emotion of the missing word. In this work, we experiment with neural Transfer Learning (TL) methods. Our models are based on LSTM networks, augmented with a self-attention mechanism. We use the weights of various pretrained models, for initializing specific layers of our networks. We leverage a big collection of unlabeled Twitter messages, for pretraining word2vec word embeddings and a set of diverse language models. Moreover, we utilize a sentiment analysis dataset for pretraining a model, which encodes emotion related information. The submitted model consists of an ensemble of the aforementioned TL models. Our team ranked 3rd out of 30 participants, achieving an F1 score of 0.703.},
}

