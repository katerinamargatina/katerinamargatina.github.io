<!DOCTYPE html>
<html>

  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<meta http-equiv="X-UA-Compatible" content="IE=edge">

<title>

  katerina margatina


</title>
<meta name="description" content="A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design.
">

<!-- Open Graph -->


<!-- Bootstrap & MDB -->
<link href="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/css/bootstrap.min.css" rel="stylesheet" integrity="sha512-MoRNloxbStBcD8z3M/2BmnT+rg4IsMxPkXaGh2zD6LGNNFE80W3onsAhRcMAMrSoyWL9xD7Ert0men7vR8LUZg==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/css/mdb.min.css" integrity="sha512-RO38pBRxYH3SoOprtPTD86JFOclM51/XTIdEPh5j8sj4tp8jmQIx26twG52UaLi//hQldfrh7e51WzP9wuP32Q==" crossorigin="anonymous" />

<!-- Fonts & Icons -->
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.14.0/css/all.min.css"  integrity="sha512-1PKOgIY59xJ8Co8+NE6FZ+LOAZKjy+KY8iq0G4B3CyeY6wYHN3yt9PW0XpSriVlkMXe40PTKnXrLnZ9+fkDaog==" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/academicons/1.9.0/css/academicons.min.css" integrity="sha512-W4yqoT1+8NLkinBLBZko+dFB2ZbHsYLDdr50VElllRcNt2Q4/GSs6u71UHKxB7S6JEMCp5Ve4xjh3eGQl/HRvg==" crossorigin="anonymous">
<link rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons">

<!-- Code Syntax Highlighting -->
<link rel="stylesheet" href="https://gitcdn.link/repo/jwarby/jekyll-pygments-themes/master/github.css" />

<!-- Styles -->

<link rel="icon" href="data:image/svg+xml,<svg xmlns=%22http://www.w3.org/2000/svg%22 viewBox=%220 0 100 100%22><text y=%22.9em%22 font-size=%2290%22>🦋</text></svg>">

<link rel="stylesheet" href="/assets/css/main.css">
<link rel="canonical" href="/">

<!-- JQuery -->
<!-- jQuery -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js" integrity="sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==" crossorigin="anonymous"></script>


<!-- Theming-->

<script src="/assets/js/theme.js"></script>
<script src="/assets/js/dark_mode.js"></script>



<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-XXXXXXXXX"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag() { dataLayer.push(arguments); }
  gtag('js', new Date());

  gtag('config', 'UA-XXXXXXXXX');
</script>



<!-- Panelbear Analytics - We respect your privacy -->
<script async src="https://cdn.panelbear.com/analytics.js?site=XXXXXXXXX"></script>
<script>
    window.panelbear = window.panelbear || function() { (window.panelbear.q = window.panelbear.q || []).push(arguments); };
    panelbear('config', { site: 'XXXXXXXXX' });
</script>


    
<!-- MathJax -->
<script type="text/javascript">
  window.MathJax = {
    tex: {
      tags: 'ams'
    }
  };
</script>
<script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.0/es5/tex-mml-chtml.js"></script>
<script defer src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>


  </head>

  <body class="fixed-top-nav ">

    <!-- Header -->

    <header>

    <!-- Nav Bar -->
    <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top">
    <div class="container">
      
      <!-- Navbar Toggle -->
      <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar top-bar"></span>
        <span class="icon-bar middle-bar"></span>
        <span class="icon-bar bottom-bar"></span>
      </button>
      <div class="collapse navbar-collapse text-right" id="navbarNav">
        <ul class="navbar-nav ml-auto flex-nowrap">
          <!-- About -->
          <li class="nav-item active">
            <a class="nav-link" href="/">
              about
              
                <span class="sr-only">(current)</span>
              
            </a>
          </li>
          
          <!-- Other pages -->
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/publications/">
                publications
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/teaching/">
                teaching
                
              </a>
          </li>
          
          
          
          
          
          <li class="nav-item ">
              <a class="nav-link" href="/vitae/">
                vitae
                
              </a>
          </li>
          
          
          
          
            <div class = "toggle-container">
              <a id = "light-toggle">
                  <i class="fas fa-moon"></i>
                  <i class="fas fa-sun"></i>
              </a>
            </div>
          
        </ul>
      </div>
    </div>
  </nav>

</header>


    <!-- Content -->

    <div class="container mt-5">
      <div class="post">

  <header class="post-header">
    <h1 class="post-title">
     <span class="font-weight-bold">Katerina</span>   Margatina
    </h1>
  </header>

  <p></p>


  <article>
    
    <div class="profile float-right">
      
        <img class="img-fluid z-depth-1 rounded" src="/assets/img/IMG_9500.JPG">
      
      
      <div class="contact-list">
        <span class="text-left">
  <a href="mailto:%6B%61%74%65%72%69%6E%61.%6D%61%72%67%61%74%69%6E%61@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a> Email
  
  <br><a href="https://scholar.google.com/citations?user=517t5gEAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a> Google Scholar
  <br><a href="https://www.semanticscholar.org/author/Katerina-Margatina/82259306" target="_blank" title="Semantic Scholar"><i class="ai ai-semantic-scholar"></i></a> Semantic Scholar
  
  
  <br><a href="https://github.com/mourga" target="_blank" title="GitHub"><i class="fab fa-github"></i></a> GitHub
  <br><a href="https://www.linkedin.com/in/katerina-margatina" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a> LinkedIn
  <br><a href="https://twitter.com/katemargatina" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a> Twitter
  
  
  
  
</span>
        <div class="contact-note"></div>
      </div>
    </div>
    

    <div class="clearfix" style="text-align: justify">
      <p>I’m currently an Applied Scientist at Amazon in NYC, working with the <a href="https://aws.amazon.com/bedrock/">AWS Bedrock</a> Agents team. My main focus is on improving how LLM agents work—making them more useful, reliable, and efficient.</p>

<p>I earned my PhD in Computer Science at the University Sheffield, under the supervision of <a href="http://nikosaletras.com/">Prof. Nikos Aletras</a>. I researched active learning algorithms for data efficient LLMs.
Along the way, I spent time as a Research Scientist intern at Meta AI (FAIR) in London where I explored the intersection of <a href="https://aclanthology.org/2023.findings-emnlp.334/">in-context and active learning methods for LLMs</a>, and at AWS in NYC where I studied <a href="https://aclanthology.org/2023.eacl-main.211/">temporal robustness of LLMs</a>.
I also visited the <a href="https://coastalcph.github.io/">CoAStaL</a> group in the University of Copenhagen, where I worked on learning from disagreement and <a href="https://aclanthology.org/2022.acl-long.482/">cross-cultural NLP</a>.</p>

<p>Before my doctoral studies (i.e., what feels like a lifetime ago), I was a Machine Learning Engineer at <a href="https://www.deepsea.ai/">DeepSea Technologies</a>. 
In my undergrad, I studied <a href="https://www.ece.ntua.gr/en">Electrical &amp; Computer Engineering</a> at the National Technical University of Athens (NTUA).</p>


    </div>

    <p></p>

    
      <div class="news">
  <h2>news</h2>
  
    <div class="table-responsive">
      <table class="table table-sm table-borderless">
      
      
        <tr>
          <th scope="row">Dec 13, 2024</th>
          <td>
            
              🌈PRISM won best <a href="https://blog.neurips.cc/2024/12/10/announcing-the-neurips-2024-best-paper-awards/">paper award at NeurIPS 2024 Datasets &amp; Benchmarks track</a>!!🚀🚀🚀

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Dec 3, 2024</th>
          <td>
            
              My PhD thesis <a href="https://etheses.whiterose.ac.uk/35849/">Exploring Active Learning Algorithms for Data Efficient Language Models</a> is finally online!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jul 22, 2024</th>
          <td>
            
              I just defended my PhD and got it with no corrections!!!🥰🎓

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Apr 24, 2024</th>
          <td>
            
              Super excited to share that our preprint <a href="https://arxiv.org/abs/2404.16019">The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</a> is on Arxiv!

            
          </td>
        </tr>
      
        <tr>
          <th scope="row">Jan 16, 2024</th>
          <td>
            
              Life update! I joined AWS Bedrock as an Applied Scientist working in LLM Agents.🤖

            
          </td>
        </tr>
      
      </table>
    </div>
  
</div>

    

    <p></p>

    
      <div class="publications">
  <h2>selected publications</h2>
  <ol class="bibliography"><li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">Thesis</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="thesis" class="col-sm-8">
        
        <div class="title">Exploring Active Learning Algorithms for Data Efficient Language Models</div>
        <div class="author">
            
            
            
            <em>Katerina Margatina</em>
            
            
            
        </div>

        <div class="periodical">
            
            
            2024
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            
            <a href="https://etheses.whiterose.ac.uk/35849/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Supervised learning is based in the premise that models can effectively solve tasks by learning from numerous examples, mapping inputs to outputs through iterative learning. However, contemporary deep learning models often require vast amounts of labeled data, termed training examples, for optimal performance. Unfortunately, not all training examples contribute equally to the learning process, leading to inefficiencies and resource wastage. Active Learning (AL) has emerged as a powerful paradigm for training language models in a data-efficient manner. By iteratively selecting informative unlabeled data points, which are then annotated by humans to form the training set, AL intelligently guides the training process, optimizing data selection for model improvement over random sampling. This thesis investigates various aspects of active learning algorithms for language mod- els, focusing on model training, data selection, in-context learning and simulation. The thesis is structured along four key publications that tackle these topics respectively. The first publication addresses the effective adaptation of pretrained language models for AL, highlighting the importance of task-specific fine-tuning. The second publication introduces a novel acquisition function, Contrastive Active Learning (CAL), which selects contrastive examples to improve AL performance. The third publication explores active learning principles for in-context learning with large language models, emphasizing the selection of informative demonstrations for few-shot learning. Lastly, the fourth publication critically examines the limitations of simulating AL experiments and pro- poses guidelines for future research. Through these contributions, this thesis aims to advance our understanding of AL algorithms for data-efficient language model training.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">NeurIPS</abbr>
        
        

        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span  style="font-size: 0.8em;" > 🏆 Best Paper</span>
        </p>
        
<!--        -->
<!--        <p>-->
<!--            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i>-->
<!--            <span  style="font-size: 0.8em;" > 🏆 Best Paper</span>-->
<!--        </p>-->
<!--        -->

        
    </div>

    <div id="prism" class="col-sm-8">
        
        <div class="title">The PRISM Alignment Project: What Participatory, Representative and Individualised Human Feedback Reveals About the Subjective and Multicultural Alignment of Large Language Models</div>
        <div class="author">
            
            
            
            
            
            Hannah Rose Kirk,
            
            
            
            
            
            
            
            
            
            Alexander Whitefield,
            
            
            
            
            
            
            
            
            
            Paul Röttger,
            
            
            
            
            
            
            
            
            
            Andrew Bean,
            
            
            
            
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Juan Ciro,
            
            
            
            
            
            
            
            
            
            Rafael Mosquera,
            
            
            
            
            
            
            
            
            
            Max Bartolo,
            
            
            
            
            
            
            
            
            
            Adina Williams,
            
            
            
            
            
            
            
            
            
            Bertie Vidgen He He,
            
            
            
            
            
            
            
            
            
            and Scott A. Hale
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Neural Information Processing Systems (NeurIPS) Track on Datasets and Benchmarks.</em>
            
            
            2024
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2404.16019" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://openreview.net/forum?id=DFr5hteojx#discussion" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            
            <a href="https://mlcommons.org/2024/05/prism/" class="btn btn-sm z-depth-0" role="button" target="_blank">Blog</a>
            
            
            <a href="https://x.com/hannahrosekirk/status/1783470502595440884" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Human feedback plays a central role in the alignment of Large Language Models (LLMs). However, open questions remain about the methods (how), domains (where), people (who) and objectives (to what end) of human feedback collection. To navigate these questions, we introduce PRISM, a new dataset which maps the sociodemographics and stated preferences of 1,500 diverse participants from 75 countries, to their contextual preferences and fine-grained feedback in 8,011 live conversations with 21 LLMs. PRISM contributes (i) wide geographic and demographic participation in human feedback data; (ii) two census-representative samples for understanding collective welfare (UK and US); and (iii) individualised feedback where every rating is linked to a detailed participant profile, thus permitting exploration of personalisation and attribution of sample artefacts. We focus on collecting conversations that centre subjective and multicultural perspectives on value-laden and controversial topics, where we expect the most interpersonal and cross-cultural disagreement. We demonstrate the usefulness of PRISM via three case studies of dialogue diversity, preference diversity, and welfare outcomes, showing that it matters which humans set alignment norms. As well as offering a rich community resource, we advocate for broader participation in AI development and a more inclusive approach to technology design.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EMNLP-Findings</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="margatina2023active" class="col-sm-8">
        
        <div class="title">Active Learning Principles for In-Context Learning with Large Language Models</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Timo Schick,
            
            
            
            
            
            
            
            
            
            Nikolaos Aletras,
            
            
            
            
            
            
            
            
            
            and Jane Dwivedi-Yu
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Findings of the Conference on Empirical Methods in Natural Language Processing (EMNLP) 2023</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2305.14264" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            
            
            
            
            <a href="https://twitter.com/katemargatina/status/1661321935127556097" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>The remarkable advancements in large language models (LLMs) have significantly enhanced the performance in few-shot learning settings. By using only a small number of labeled examples, referred to as demonstrations, LLMs can effectively grasp the task at hand through in-context learning. However, the process of selecting appropriate demonstrations has received limited attention in prior work. This paper addresses the issue of identifying the most informative demonstrations for few-shot learning by approaching it as a pool-based Active Learning (AL) problem over a single iteration. Our objective is to investigate how AL algorithms can serve as effective demonstration selection methods for in-context learning. We compare various standard AL algorithms based on uncertainty, diversity, and similarity, and consistently observe that the latter outperforms all other methods, including random sampling. Notably, uncertainty sampling, despite its success in conventional supervised learning scenarios, performs poorly in this context. Our extensive experimentation involving a diverse range of GPT and OPT models across 24 classification and multi-choice tasks, coupled with thorough analysis, unambiguously demonstrates that in-context example selection through AL prioritizes high-quality examples that exhibit low uncertainty and bear similarity to the test examples.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">ACL-Findings</abbr>
        
        

        
<!--        -->

        
    </div>

    <div id="margatina2023limitations" class="col-sm-8">
        
        <div class="title">On the Limitations of Simulating Active Learning</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            and Nikolaos Aletras
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Findings of the Association for Computational Linguistics (ACL)</em>
            
            
            2023
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2305.13342" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2023.findings-acl.269/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2023.findings-acl.269.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Active learning (AL) is a human-and-model-in-the-loop paradigm that iteratively selects informative unlabeled data for human annotation, aiming to improve over random sampling. However, performing AL experiments with human annotations on-the-fly is a laborious and expensive process, thus unrealistic for academic research. An easy fix to this impediment is to simulate AL, by treating an already labeled and publicly available dataset as the pool of unlabeled data. In this position paper, we first survey recent literature and highlight the challenges across all different steps within the AL loop. We further unveil neglected caveats in the experimental setup that can significantly affect the quality of AL research. We continue with an exploration of how the simulation setting can govern empirical findings, arguing that it might be one of the answers behind the ever posed question “why do active learning algorithms sometimes fail to outperform random sampling?”. We argue that evaluating AL algorithms on available labeled datasets might provide a lower bound as to their effectiveness in real data. We believe it is essential to collectively shape the best practices for AL research, particularly as engineering advancements in LLMs push the research focus towards data-driven approaches (e.g., data efficiency, alignment, fairness). In light of this, we have developed guidelines for future work. Our aim is to draw attention to these limitations within the community, in the hope of finding ways to address them.</p>
        </div>
        
    </div>
</div></li>
<li><div class="row">
    <div class="col-sm-2 abbr">
        
        
        <abbr class="badge">EMNLP</abbr>
        
        

        
        <p>
            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.5em;" aria-label="TROPHY"></i>
            <span  style="font-size: 0.8em;" >  ✨ Oral ✨</span>
        </p>
        
<!--        -->
<!--        <p>-->
<!--            <i class="em em-trophy" aria-role="presentation" style="font-size: 0.7em;" aria-label="TROPHY"></i>-->
<!--            <span  style="font-size: 0.8em;" >  ✨ Oral ✨</span>-->
<!--        </p>-->
<!--        -->

        
    </div>

    <div id="cal" class="col-sm-8">
        
        <div class="title">Active Learning by Acquiring Contrastive Examples</div>
        <div class="author">
            
            
            
            
            <em>Katerina Margatina</em>,
            
            
            
            
            
            
            
            
            Giorgos Vernikos,
            
            
            
            
            
            
            
            
            
            Loïc Barrault,
            
            
            
            
            
            
            
            
            
            and Nikolaos Aletras
            
            
            
            
            
        </div>

        <div class="periodical">
            
            <em>In Proceedings of the Conference on Empirical Methods in Natural Language Processing (EMNLP)</em>
            
            
            2021
            
        </div>
        

        <div class="links">
            
            <a class="abstract btn btn-sm z-depth-0" role="button">Abstract</a>
            
            
            <a href="http://arxiv.org/abs/2109.03764" class="btn btn-sm z-depth-0" role="button" target="_blank">arXiv</a>
            
            
            <a href="https://aclanthology.org/2021.emnlp-main.51/" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-pager"></i> HTML</a>
            
            
            
            <a href="https://aclanthology.org/2021.emnlp-main.51.pdf" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="far fa-file-pdf"></i> PDF</a>
            
            
            
            
            
            <a href="https://twitter.com/katemargatina/status/1437393852227276801?s=20" class="btn btn-sm z-depth-0" role="button" target="_blank">TL;DR</a>
            
            
            <a href="https://github.com/mourga/contrastive-active-learning" class="btn btn-sm z-depth-0" role="button" target="_blank"><i class="fas fa-code"></i> Code</a>
            
            
            
            <a href="https://drive.google.com/file/d/1JAdukTFb0ceAR0WRk9zEW2GqLiTG_p7H/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">Poster</a>
            
            
            
            
            <a href="https://drive.google.com/file/d/1bWx9QPRTFjf49XLgCdV34w3w1SXETylv/view?usp=sharing" class="btn btn-sm z-depth-0" role="button" target="_blank">
                <i class="far fa-file-powerpoint"></i>
                Slides</a>
            
            
            
        </div>

        <!-- Hidden abstract block -->
        
        <div class="abstract hidden">
            <p>Common acquisition functions for active learning use either uncertainty or diversity sampling, aiming to select difficult and diverse data points from the pool of unlabeled data, respectively. In this work, leveraging the best of both worlds, we propose an acquisition function that opts for selecting \textitcontrastive examples, i.e. data points that are similar in the model feature space and yet the model outputs maximally different predictive likelihoods. We compare our approach, CAL (Contrastive Active Learning), with a diverse set of acquisition functions in four natural language understanding tasks and seven datasets. Our experiments show that CAL performs consistently better or equal than the best performing baseline across all tasks, on both in-domain and out-of-domain data. We also conduct an extensive ablation study of our method and we further analyze all actively acquired datasets showing that CAL achieves a better trade-off between uncertainty and diversity compared to other strategies.</p>
        </div>
        
    </div>
</div></li></ol>
</div>

    

    
    <div class="social">
      <a href="mailto:%6B%61%74%65%72%69%6E%61.%6D%61%72%67%61%74%69%6E%61@%67%6D%61%69%6C.%63%6F%6D"><i class="fas fa-envelope"></i></a>

<a href="https://scholar.google.com/citations?user=517t5gEAAAAJ" target="_blank" title="Google Scholar"><i class="ai ai-google-scholar"></i></a>
<a href="https://www.semanticscholar.org/author/Katerina-Margatina/82259306" target="_blank" title="Semantic Scholar"><i class="ai ai-semantic-scholar"></i></a>


<a href="https://github.com/mourga" target="_blank" title="GitHub"><i class="fab fa-github"></i></a>
<a href="https://www.linkedin.com/in/katerina-margatina" target="_blank" title="LinkedIn"><i class="fab fa-linkedin"></i></a>
<a href="https://twitter.com/katemargatina" target="_blank" title="Twitter"><i class="fab fa-twitter"></i></a>










      <div class="contact-note"></div>
    </div>
    
  </article>

</div>
    </div>

    <!-- Footer -->

    
<footer class="fixed-bottom">
  <div class="container mt-0">
    &copy; Copyright 2024 Katerina  Margatina.
    Powered by <a href="http://jekyllrb.com/" target="_blank">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank">GitHub Pages</a>.

    
    
  </div>
</footer>



  </body>

  <!-- Bootsrap & MDB scripts -->
<script src="https://cdnjs.cloudflare.com/ajax/libs/popper.js/2.4.4/umd/popper.min.js" integrity="sha512-eUQ9hGdLjBjY3F41CScH3UX+4JDSI9zXeroz7hJ+RteoCaY+GP/LDoM8AO+Pt+DRFw3nXqsjh9Zsts8hnYv8/A==" crossorigin="anonymous"></script>
<script src="https://stackpath.bootstrapcdn.com/bootstrap/4.5.2/js/bootstrap.min.js" integrity="sha512-M5KW3ztuIICmVIhjSqXe01oV2bpe248gOxqmlcYrEzAvws7Pw3z6BK0iGbrwvdrUQUhi3eXgtxp5I8PDo9YfjQ==" crossorigin="anonymous"></script>
<script src="https://cdnjs.cloudflare.com/ajax/libs/mdbootstrap/4.19.1/js/mdb.min.js" integrity="sha512-Mug9KHKmroQFMLm93zGrjhibM2z2Obg9l6qFG2qKjXEXkMp/VDkI4uju9m4QKPjWSwQ6O2qzZEnJDEeCw0Blcw==" crossorigin="anonymous"></script>

  
<!-- Mansory & imagesLoaded -->
<script defer src="https://unpkg.com/masonry-layout@4/dist/masonry.pkgd.min.js"></script>
<script defer src="https://unpkg.com/imagesloaded@4/imagesloaded.pkgd.min.js"></script>
<script defer src="/assets/js/mansory.js" type="text/javascript"></script>


  
<!-- Enable Tooltips -->
<script type="text/javascript">
$(function () {$('[data-toggle="tooltip"]').tooltip()})
</script>



<!-- Medium Zoom JS -->
<script src="https://cdn.jsdelivr.net/npm/medium-zoom@1.0.6/dist/medium-zoom.min.js" integrity="sha256-EdPgYcPk/IIrw7FYeuJQexva49pVRZNmt3LculEr7zM=" crossorigin="anonymous"></script>
<script src="/assets/js/zoom.js"></script>


<!-- Load Common JS -->
<script src="/assets/js/common.js"></script>


</html>
